# ============================================
# Makefile - Sprint 1 NLU Foundation
# ============================================

.PHONY: help setup install clean dataset train serve test docker-up docker-down mlflow logs

# Variables
PYTHON := python
PIP := pip
VENV := venv
BACKEND := backend
DATA_DIR := $(BACKEND)/data
MODELS_DIR := $(BACKEND)/models

# Couleurs pour output
RED := \033[0;31m
GREEN := \033[0;32m
YELLOW := \033[1;33m
BLUE := \033[0;34m
NC := \033[0m # No Color

# ============================================
# Aide
# ============================================

help: ## Afficher cette aide
	@echo "$(BLUE)â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—$(NC)"
	@echo "$(BLUE)â•‘    Sprint 1 - NLU Foundation Setup      â•‘$(NC)"
	@echo "$(BLUE)â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•$(NC)"
	@echo ""
	@echo "$(GREEN)Commandes disponibles:$(NC)"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | \
		awk 'BEGIN {FS = ":.*?## "}; {printf "  $(YELLOW)%-20s$(NC) %s\n", $$1, $$2}'
	@echo ""
	@echo "$(GREEN)Workflow complet:$(NC)"
	@echo "  1. $(YELLOW)make setup$(NC)      - Setup initial"
	@echo "  2. $(YELLOW)make docker-up$(NC)  - DÃ©marrer services"
	@echo "  3. $(YELLOW)make dataset$(NC)    - CrÃ©er dataset"
	@echo "  4. $(YELLOW)make train$(NC)      - EntraÃ®ner modÃ¨le"
	@echo "  5. $(YELLOW)make serve$(NC)      - Lancer API"
	@echo "  6. $(YELLOW)make test$(NC)       - Tests"

# ============================================
# Setup & Installation
# ============================================

setup: ## Setup complet du projet
	@echo "$(GREEN)ğŸš€ Setup du projet...$(NC)"
	@make install
	@make docker-up
	@echo "$(GREEN)âœ… Setup terminÃ©!$(NC)"

install: ## Installer dÃ©pendances Python
	@echo "$(GREEN)ğŸ“¦ Installation des dÃ©pendances...$(NC)"
	cd $(BACKEND) && \
		$(PYTHON) -m venv $(VENV) && \
		. $(VENV)/bin/activate && \
		$(PIP) install --upgrade pip && \
		$(PIP) install -r requirements.txt && \
		$(PYTHON) -m spacy download fr_core_news_lg
	@echo "$(GREEN)âœ… DÃ©pendances installÃ©es$(NC)"

install-dev: ## Installer dÃ©pendances de dev
	@echo "$(GREEN)ğŸ“¦ Installation dÃ©pendances dev...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		$(PIP) install black isort flake8 mypy pytest pytest-cov pytest-asyncio
	@echo "$(GREEN)âœ… DÃ©pendances dev installÃ©es$(NC)"

# ============================================
# Docker Services
# ============================================

docker-up: ## DÃ©marrer services Docker (PostgreSQL, Redis, MLflow)
	@echo "$(GREEN)ğŸ³ DÃ©marrage des services Docker...$(NC)"
	docker-compose up -d
	@echo "$(YELLOW)â³ Attente dÃ©marrage services (10s)...$(NC)"
	@sleep 10
	@make docker-status
	@echo "$(GREEN)âœ… Services dÃ©marrÃ©s$(NC)"

docker-down: ## ArrÃªter services Docker
	@echo "$(YELLOW)ğŸ›‘ ArrÃªt des services Docker...$(NC)"
	docker-compose down
	@echo "$(GREEN)âœ… Services arrÃªtÃ©s$(NC)"

docker-restart: ## RedÃ©marrer services Docker
	@make docker-down
	@make docker-up

docker-status: ## VÃ©rifier status services Docker
	@echo "$(BLUE)ğŸ“Š Status des services:$(NC)"
	@docker-compose ps

docker-logs: ## Voir logs Docker
	docker-compose logs -f

docker-clean: ## Nettoyer containers et volumes Docker
	@echo "$(RED)âš ï¸  Nettoyage complet Docker (volumes supprimÃ©s)$(NC)"
	@read -p "Continuer? (y/N) " confirm && [ "$$confirm" = "y" ] || exit 1
	docker-compose down -v
	@echo "$(GREEN)âœ… Nettoyage terminÃ©$(NC)"

# ============================================
# Dataset
# ============================================

dataset: ## CrÃ©er le dataset avec augmentation
	@echo "$(GREEN)ğŸ“Š CrÃ©ation du dataset...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		$(PYTHON) data/create_dataset.py
	@echo "$(GREEN)âœ… Dataset crÃ©Ã©: $(DATA_DIR)/processed/$(NC)"
	@make dataset-stats

dataset-stats: ## Afficher stats du dataset
	@echo "$(BLUE)ğŸ“ˆ Statistiques du dataset:$(NC)"
	@cd $(BACKEND) && cat data/processed/metadata.json | $(PYTHON) -m json.tool

dataset-view: ## Voir quelques exemples du dataset
	@echo "$(BLUE)ğŸ‘€ Exemples du dataset:$(NC)"
	@cd $(BACKEND) && head -n 5 data/processed/train.json

dataset-clean: ## Supprimer le dataset
	@echo "$(YELLOW)ğŸ—‘ï¸  Suppression du dataset...$(NC)"
	rm -rf $(DATA_DIR)/processed
	@echo "$(GREEN)âœ… Dataset supprimÃ©$(NC)"

# ============================================
# Training
# ============================================

train: ## EntraÃ®ner le modÃ¨le
	@echo "$(GREEN)ğŸ“ EntraÃ®nement du modÃ¨le...$(NC)"
	@if [ ! -d "$(DATA_DIR)/processed" ]; then \
		echo "$(RED)âŒ Dataset non trouvÃ©. Lancez 'make dataset' d'abord$(NC)"; \
		exit 1; \
	fi
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		$(PYTHON) -m app.services.training_service
	@echo "$(GREEN)âœ… EntraÃ®nement terminÃ©$(NC)"
	@make train-stats

train-stats: ## Afficher stats du modÃ¨le entraÃ®nÃ©
	@echo "$(BLUE)ğŸ“Š ModÃ¨le entraÃ®nÃ©:$(NC)"
	@if [ -f "$(MODELS_DIR)/best_model/config.json" ]; then \
		cat $(MODELS_DIR)/best_model/config.json | $(PYTHON) -m json.tool; \
	else \
		echo "$(RED)âŒ Aucun modÃ¨le trouvÃ©$(NC)"; \
	fi

train-clean: ## Supprimer modÃ¨les entraÃ®nÃ©s
	@echo "$(YELLOW)ğŸ—‘ï¸  Suppression des modÃ¨les...$(NC)"
	rm -rf $(MODELS_DIR)/best_model
	rm -rf $(MODELS_DIR)/confusion_matrix.png
	@echo "$(GREEN)âœ… ModÃ¨les supprimÃ©s$(NC)"

# ============================================
# Serving
# ============================================

serve: ## Lancer l'API FastAPI
	@echo "$(GREEN)ğŸš€ DÃ©marrage de l'API...$(NC)"
	@if [ ! -d "$(MODELS_DIR)/best_model" ]; then \
		echo "$(RED)âŒ ModÃ¨le non trouvÃ©. Lancez 'make train' d'abord$(NC)"; \
		exit 1; \
	fi
	@echo "$(BLUE)ğŸ“¡ API disponible sur: http://localhost:8000$(NC)"
	@echo "$(BLUE)ğŸ“š Docs: http://localhost:8000/docs$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		$(PYTHON) run.py

serve-prod: ## Lancer en mode production (Gunicorn)
	@echo "$(GREEN)ğŸš€ DÃ©marrage en mode production...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		gunicorn app.main:app \
			--workers 4 \
			--worker-class uvicorn.workers.UvicornWorker \
			--bind 0.0.0.0:8000 \
			--log-level info

# ============================================
# Testing
# ============================================

test: ## Lancer tous les tests
	@echo "$(GREEN)ğŸ§ª Lancement des tests...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		pytest tests/ -v

test-unit: ## Tests unitaires seulement
	@echo "$(GREEN)ğŸ§ª Tests unitaires...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		pytest tests/test_intent_classifier.py tests/test_nlu_service.py -v

test-api: ## Tests API seulement
	@echo "$(GREEN)ğŸ§ª Tests API...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		pytest tests/test_api.py -v

test-integration: ## Tests d'intÃ©gration
	@echo "$(GREEN)ğŸ§ª Tests d'intÃ©gration...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		pytest tests/test_integration.py -v

test-cov: ## Tests avec coverage
	@echo "$(GREEN)ğŸ§ª Tests avec coverage...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		pytest tests/ --cov=app --cov-report=html --cov-report=term
	@echo "$(BLUE)ğŸ“Š Rapport: $(BACKEND)/htmlcov/index.html$(NC)"

test-watch: ## Tests en mode watch (reruns on changes)
	@echo "$(GREEN)ğŸ§ª Tests en mode watch...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		pytest-watch tests/ -v

# ============================================
# MLflow
# ============================================

mlflow: ## Ouvrir MLflow UI
	@echo "$(BLUE)ğŸ“Š MLflow UI: http://localhost:5000$(NC)"
	@open http://localhost:5000 || xdg-open http://localhost:5000 || start http://localhost:5000

mlflow-clean: ## Nettoyer expÃ©riences MLflow
	@echo "$(YELLOW)ğŸ—‘ï¸  Nettoyage MLflow...$(NC)"
	docker-compose exec mlflow rm -rf /mlflow/*
	@echo "$(GREEN)âœ… MLflow nettoyÃ©$(NC)"

# ============================================
# Code Quality
# ============================================

format: ## Formater le code (Black + isort)
	@echo "$(GREEN)âœ¨ Formatage du code...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		black app/ tests/ && \
		isort app/ tests/
	@echo "$(GREEN)âœ… Code formatÃ©$(NC)"

lint: ## Linter (flake8 + mypy)
	@echo "$(GREEN)ğŸ” Linting...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		flake8 app/ tests/ --max-line-length=120 && \
		mypy app/ --ignore-missing-imports
	@echo "$(GREEN)âœ… Linting OK$(NC)"

check: format lint test ## Format + Lint + Test

# ============================================
# API Testing
# ============================================

api-test: ## Tester l'API manuellement
	@echo "$(GREEN)ğŸ”§ Tests API manuels...$(NC)"
	@echo ""
	@echo "$(BLUE)1. Health check:$(NC)"
	@curl -s http://localhost:8000/health | $(PYTHON) -m json.tool
	@echo ""
	@echo "$(BLUE)2. Classification:$(NC)"
	@curl -s -X POST http://localhost:8000/api/nlp/classify \
		-H "Content-Type: application/json" \
		-d '{"text": "OÃ¹ est ma commande ?"}' | $(PYTHON) -m json.tool
	@echo ""
	@echo "$(BLUE)3. Stats:$(NC)"
	@curl -s http://localhost:8000/api/nlp/stats | $(PYTHON) -m json.tool

api-benchmark: ## Benchmark de l'API
	@echo "$(GREEN)âš¡ Benchmark de l'API...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		$(PYTHON) -c "import asyncio; \
			from httpx import AsyncClient; \
			import time; \
			async def bench(): \
				async with AsyncClient(base_url='http://localhost:8000') as client: \
					start = time.time(); \
					tasks = [client.post('/api/nlp/classify', json={'text': 'test'}) for _ in range(100)]; \
					await asyncio.gather(*tasks); \
					print(f'100 requÃªtes en {time.time()-start:.2f}s'); \
			asyncio.run(bench())"

# ============================================
# Logs & Monitoring
# ============================================

logs: ## Voir logs backend
	@tail -f $(BACKEND)/logs/*.log 2>/dev/null || echo "Pas de logs"

logs-redis: ## Logs Redis
	docker-compose logs -f redis

logs-postgres: ## Logs PostgreSQL
	docker-compose logs -f postgres

logs-mlflow: ## Logs MLflow
	docker-compose logs -f mlflow

# ============================================
# Utilities
# ============================================

notebook: ## Lancer Jupyter notebook
	@echo "$(GREEN)ğŸ““ Lancement Jupyter...$(NC)"
	cd notebooks && \
		. ../$(BACKEND)/$(VENV)/bin/activate && \
		jupyter notebook

shell: ## Shell Python avec env activÃ©
	@echo "$(GREEN)ğŸš Python shell...$(NC)"
	cd $(BACKEND) && \
		. $(VENV)/bin/activate && \
		$(PYTHON)

redis-cli: ## AccÃ©der Ã  Redis CLI
	@echo "$(GREEN)ğŸ”´ Redis CLI...$(NC)"
	docker exec -it chatbot_redis redis-cli

psql: ## AccÃ©der Ã  PostgreSQL
	@echo "$(GREEN)ğŸ˜ PostgreSQL CLI...$(NC)"
	docker exec -it chatbot_postgres psql -U postgres -d chatbot_db

# ============================================
# Clean & Reset
# ============================================

clean: ## Nettoyer fichiers temporaires
	@echo "$(YELLOW)ğŸ§¹ Nettoyage...$(NC)"
	find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	find . -type f -name "*.pyc" -delete
	find . -type f -name "*.pyo" -delete
	find . -type f -name "*.coverage" -delete
	find . -type d -name "*.egg-info" -exec rm -rf {} + 2>/dev/null || true
	find . -type d -name ".pytest_cache" -exec rm -rf {} + 2>/dev/null || true
	rm -rf $(BACKEND)/htmlcov
	@echo "$(GREEN)âœ… Nettoyage terminÃ©$(NC)"

reset: clean dataset-clean train-clean docker-clean ## Reset complet du projet
	@echo "$(RED)âš ï¸  Reset complet effectuÃ©$(NC)"

# ============================================
# Status & Info
# ============================================

status: ## VÃ©rifier status complet
	@echo "$(BLUE)â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—$(NC)"
	@echo "$(BLUE)â•‘          Status du Projet Sprint 1      â•‘$(NC)"
	@echo "$(BLUE)â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•$(NC)"
	@echo ""
	@echo "$(GREEN)ğŸ“¦ Services Docker:$(NC)"
	@make docker-status
	@echo ""
	@echo "$(GREEN)ğŸ“Š Dataset:$(NC)"
	@if [ -d "$(DATA_DIR)/processed" ]; then \
		echo "  âœ… Dataset prÃ©sent"; \
		cat $(DATA_DIR)/processed/metadata.json | $(PYTHON) -c "import sys,json; d=json.load(sys.stdin); print(f'  ğŸ“ˆ Train: {d[\"train_size\"]}, Val: {d[\"val_size\"]}, Test: {d[\"test_size\"]}')"; \
	else \
		echo "  âŒ Dataset absent (lancez 'make dataset')"; \
	fi
	@echo ""
	@echo "$(GREEN)ğŸ“ ModÃ¨le:$(NC)"
	@if [ -d "$(MODELS_DIR)/best_model" ]; then \
		echo "  âœ… ModÃ¨le entraÃ®nÃ© prÃ©sent"; \
	else \
		echo "  âŒ ModÃ¨le absent (lancez 'make train')"; \
	fi
	@echo ""
	@echo "$(GREEN)ğŸ”— URLs utiles:$(NC)"
	@echo "  ğŸ“¡ API: http://localhost:8000"
	@echo "  ğŸ“š Docs: http://localhost:8000/docs"
	@echo "  ğŸ“Š MLflow: http://localhost:5000"

version: ## Afficher versions
	@echo "$(BLUE)ğŸ“Œ Versions:$(NC)"
	@$(PYTHON) --version
	@docker --version
	@docker-compose --version

# ============================================
# Quick Start
# ============================================

quickstart: setup dataset train ## Setup + Dataset + Train (tout en un)
	@echo ""
	@echo "$(GREEN)â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—$(NC)"
	@echo "$(GREEN)â•‘   âœ… Quickstart terminÃ© avec succÃ¨s!     â•‘$(NC)"
	@echo "$(GREEN)â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•$(NC)"
	@echo ""
	@echo "$(YELLOW)Prochaines Ã©tapes:$(NC)"
	@echo "  1. $(BLUE)make serve$(NC)     - Lancer l'API"
	@echo "  2. $(BLUE)make api-test$(NC)  - Tester l'API"
	@echo "  3. $(BLUE)make test$(NC)      - Lancer les tests"
	@echo "  4. $(BLUE)make mlflow$(NC)    - Voir les mÃ©triques"

# Default target
.DEFAULT_GOAL := help